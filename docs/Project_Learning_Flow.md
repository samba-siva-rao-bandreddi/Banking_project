# ğŸ—ºï¸ Banking Data Pipeline â€” Project Learning Flow

## ğŸ“Œ What Does This Project Do? (One-Line Summary)

**Generates fake banking data â†’ stores it in PostgreSQL â†’ captures every change via CDC â†’ streams it through Kafka â†’ uploads to AWS S3 as Parquet files â†’ transforms it in Snowflake using dbt â†’ all orchestrated by Apache Airflow.**

---

## ğŸ—ï¸ Complete Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        YOUR WINDOWS MACHINE                            â”‚
â”‚                                                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                     â”‚
â”‚   â”‚    .env      â”‚ â† All passwords, keys, and config live here         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                     â”‚
â”‚          â”‚ (read by every component)                                    â”‚
â”‚          â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DOCKER COMPOSE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                                                                  â”‚  â”‚
â”‚   â”‚  STEP 1: Generate Fake Data                                      â”‚  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚   â”‚  â”‚  fake_generator   â”‚â”€â”€â”€â”€â”€â–¶â”‚  PostgreSQL (5432)  â”‚              â”‚  â”‚
â”‚   â”‚  â”‚  (Python/Faker)   â”‚      â”‚  Banking Database    â”‚              â”‚  â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  â€¢ customers         â”‚              â”‚  â”‚
â”‚   â”‚  Runs on YOUR machine       â”‚  â€¢ accounts          â”‚              â”‚  â”‚
â”‚   â”‚  (not in Docker)            â”‚  â€¢ transactions      â”‚              â”‚  â”‚
â”‚   â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚
â”‚   â”‚                                        â”‚                         â”‚  â”‚
â”‚   â”‚  STEP 2: Capture Changes (CDC)         â”‚ WAL (Write-Ahead Log)  â”‚  â”‚
â”‚   â”‚                                        â–¼                         â”‚  â”‚
â”‚   â”‚                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚   â”‚                             â”‚  Debezium Connect    â”‚              â”‚  â”‚
â”‚   â”‚                             â”‚  (port 8083)         â”‚              â”‚  â”‚
â”‚   â”‚                             â”‚  Reads every change  â”‚              â”‚  â”‚
â”‚   â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚
â”‚   â”‚                                        â”‚                         â”‚  â”‚
â”‚   â”‚  STEP 3: Stream Events                 â”‚ JSON events             â”‚  â”‚
â”‚   â”‚                                        â–¼                         â”‚  â”‚
â”‚   â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚   â”‚             â”‚Zookeeper â”‚â—„â”€â”€â”€â”‚  Apache Kafka        â”‚              â”‚  â”‚
â”‚   â”‚             â”‚(manager) â”‚    â”‚  (ports 9092/29092)  â”‚              â”‚  â”‚
â”‚   â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  Topics:             â”‚              â”‚  â”‚
â”‚   â”‚                             â”‚  â€¢ banking_server.   â”‚              â”‚  â”‚
â”‚   â”‚                             â”‚    public.customers  â”‚              â”‚  â”‚
â”‚   â”‚                             â”‚  â€¢ banking_server.   â”‚              â”‚  â”‚
â”‚   â”‚                             â”‚    public.accounts   â”‚              â”‚  â”‚
â”‚   â”‚                             â”‚  â€¢ banking_server.   â”‚              â”‚  â”‚
â”‚   â”‚                             â”‚    public.transactionsâ”‚             â”‚  â”‚
â”‚   â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚
â”‚   â”‚                                        â”‚                         â”‚  â”‚
â”‚   â”‚  STEP 4: Consume & Upload              â”‚                         â”‚  â”‚
â”‚   â”‚                                        â–¼                         â”‚  â”‚
â”‚   â”‚                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚   â”‚                             â”‚  kafka_to_s3.py      â”‚              â”‚  â”‚
â”‚   â”‚                             â”‚  (Python consumer)   â”‚              â”‚  â”‚
â”‚   â”‚                             â”‚  Reads Kafka events  â”‚              â”‚  â”‚
â”‚   â”‚                             â”‚  Writes .parquet     â”‚              â”‚  â”‚
â”‚   â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚
â”‚   â”‚                                        â”‚                         â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                            â”‚                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                                             â–¼
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚  AWS S3 (Cloud)      â”‚
                                  â”‚  Data Lake            â”‚
                                  â”‚  â€¢ customers/         â”‚
                                  â”‚  â€¢ accounts/          â”‚
                                  â”‚  â€¢ transactions/      â”‚
                                  â”‚  (Parquet files)      â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                    STEP 5: Transform         â”‚
                                             â–¼
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚  Snowflake (Cloud)   â”‚
                                  â”‚  + dbt models        â”‚
                                  â”‚  (Clean & transform) â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                    STEP 6: Orchestrate       â”‚
                                             â–¼
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚  Apache Airflow      â”‚
                                  â”‚  (port 8080)         â”‚
                                  â”‚  Schedules & monitorsâ”‚
                                  â”‚  the entire pipeline â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ File Map â€” Every File and What It Does

```
project/
â”œâ”€â”€ .env                          â† ğŸ”‘ ALL secrets and config (DB passwords, AWS keys)
â”‚
â””â”€â”€ Banking_project/
    â”œâ”€â”€ docker-compose.yml        â† ğŸ³ Defines all 8 Docker containers
    â”œâ”€â”€ docker-airflow.dockerfile â† ğŸ—ï¸ Custom Airflow image (installs dbt)
    â”œâ”€â”€ requirements.txt          â† ğŸ“¦ Python dependencies list
    â”‚
    â”œâ”€â”€ postgres/
    â”‚   â””â”€â”€ schema.sql            â† ğŸ“‹ Creates the 3 database tables
    â”‚
    â”œâ”€â”€ data_generator/
    â”‚   â””â”€â”€ fake_generator.py     â† ğŸ² Generates fake banking data â†’ inserts into Postgres
    â”‚
    â”œâ”€â”€ kafka/
    â”‚   â””â”€â”€ generate_and_post.py  â† ğŸ”— Registers Debezium connector via REST API
    â”‚
    â”œâ”€â”€ AWS_S3/
    â”‚   â””â”€â”€ kafka_to_s3.py        â† â˜ï¸ Kafka consumer â†’ reads events â†’ uploads Parquet to S3
    â”‚
    â”œâ”€â”€ test_kafka.py             â† ğŸ§ª Test script: check if Kafka topics have messages
    â”œâ”€â”€ test_s3.py                â† ğŸ§ª Test script: check if S3 connection works
    â”‚
    â””â”€â”€ docs/
        â”œâ”€â”€ Docker_Compose_Explained.md  â† ğŸ“š Line-by-line docker-compose explanation
        â”œâ”€â”€ AWS_S3_Setup_Guide.md        â† ğŸ“š How to set up AWS S3
        â””â”€â”€ Kafka_to_S3_Architecture.md  â† ğŸ“š Architecture documentation
```

---

## ğŸ“ Learning Flow â€” Where to Start and in What Order

### Phase 1: Configuration (Understand the Foundation)

#### Step 1 â†’ `.env` file
ğŸ“ **File:** `project/.env`

**What to learn:** This is the configuration brain. Every other file reads from here.

```env
# Banking Database
POSTGRES_HOST=localhost          â† Where Postgres is running
POSTGRES_PORT=5432               â† Which port
POSTGRES_USER=postgres           â† Login username
POSTGRES_PASSWORD=raptee123      â† Login password
POSTGRES_DB=project              â† Database name

# Airflow Database (separate!)
AIRFLOW_DB_USER=airflow
AIRFLOW_DB_PASSWORD=airflow
AIRFLOW_DB_NAME=airflow

# AWS S3 (cloud storage)
AWS_ACCESS_KEY_ID=your_key
AWS_SECRET_ACCESS_KEY=your_secret
AWS_REGION=your_region
S3_BUCKET_NAME=your_bucket
```

**Key concept:** Environment variables keep secrets OUT of your code. If you push to GitHub, `.env` is in `.gitignore` so your passwords don't leak.

---

### Phase 2: Database (Where Data Lives)

#### Step 2 â†’ `postgres/schema.sql`
ğŸ“ **File:** `Banking_project/postgres/schema.sql`

**What to learn:** The 3 tables that store everything.

```
customers â”€â”€â”€â”€â”€â”€â”
  â€¢ id           â”‚
  â€¢ first_name   â”‚ one customer has
  â€¢ last_name    â”‚ many accounts
  â€¢ email        â”‚
                 â–¼
accounts â”€â”€â”€â”€â”€â”€â”€â”
  â€¢ id           â”‚
  â€¢ customer_id  â”‚ one account has
  â€¢ account_type â”‚ many transactions
  â€¢ balance      â”‚
  â€¢ currency     â”‚
                 â–¼
transactions
  â€¢ id
  â€¢ account_id
  â€¢ tnx_type (DEPOSIT/WITHDRAWAL/TRANSFER)
  â€¢ amount
  â€¢ related_acc_id (for transfers)
  â€¢ status
```

**Key concept:** Foreign keys link the tables. `accounts.customer_id â†’ customers.id` and `transactions.account_id â†’ accounts.id`. The `ON DELETE CASCADE` means if you delete a customer, all their accounts and transactions are also deleted.

---

### Phase 3: Data Generation (Creating Fake Data)

#### Step 3 â†’ `data_generator/fake_generator.py`
ğŸ“ **File:** `Banking_project/data_generator/fake_generator.py`

**What to learn:** How fake data is created and inserted.

**What it does step by step:**
1. Connects to PostgreSQL using credentials from `.env`
2. Creates 10 fake customers (using the `Faker` library)
3. Creates 2 accounts per customer (SAVINGS or CHECKING)
4. Creates 50 random transactions (DEPOSIT, WITHDRAWAL, TRANSFER)
5. **Loops every 3 seconds** to keep generating data (this simulates real-world banking activity)

**Key concept:** This script runs on YOUR Windows machine (not inside Docker). It connects to Postgres via `localhost:5432`.

**How to run:**
```bash
python data_generator/fake_generator.py          # loops forever
python data_generator/fake_generator.py --once   # runs once and stops
```

---

### Phase 4: Infrastructure (Docker Containers)

#### Step 4 â†’ `docker-compose.yml`
ğŸ“ **File:** `Banking_project/docker-compose.yml`
ğŸ“š **Detailed explanation:** `docs/Docker_Compose_Explained.md`

**What to learn:** This starts 8 containers:

| # | Container | Port | Purpose |
|---|-----------|------|---------|
| 1 | Zookeeper | 2181 | Manages Kafka |
| 2 | Kafka | 9092, 29092 | Message broker |
| 3 | Debezium Connect | 8083 | Watches Postgres for changes |
| 4 | PostgreSQL (Banking) | 5432 | Stores banking data |
| 5 | Airflow Init | â€” | One-time setup (exits after) |
| 6 | Airflow Webserver | 8080 | Pipeline UI |
| 7 | Airflow Scheduler | â€” | Runs scheduled tasks |
| 8 | Airflow Postgres | 5433 | Airflow's internal database |

**How to run:**
```bash
cd Banking_project
docker compose up -d      # start all containers in background
docker compose ps         # check which containers are running
docker compose logs kafka # see logs for a specific container
docker compose down       # stop everything
```

#### Step 4b â†’ `docker-airflow.dockerfile`
ğŸ“ **File:** `Banking_project/docker-airflow.dockerfile`

**What to learn:** Only 3 lines â€” builds a custom Airflow image with dbt installed.

```dockerfile
FROM apache/airflow:2.9.3    # Start from official Airflow image
USER airflow                 # Switch to the airflow user
RUN pip install dbt-core dbt-snowflake   # Install dbt packages
```

---

### Phase 5: CDC â€” Change Data Capture (The Magic)

#### Step 5 â†’ `kafka/generate_and_post.py`
ğŸ“ **File:** `Banking_project/kafka/generate_and_post.py`

**What to learn:** This is where CDC gets activated.

**What it does:**
1. Builds a JSON configuration for the Debezium Postgres connector
2. Sends a POST request to Debezium's REST API at `http://localhost:8083/connectors`
3. Debezium then starts watching these tables: `customers`, `accounts`, `transactions`

**After this script runs, the CDC flow is:**
```
Any INSERT/UPDATE/DELETE in Postgres
        â†“
Debezium reads the WAL log
        â†“
Converts to JSON event
        â†“
Publishes to Kafka topics:
  â€¢ banking_server.public.customers
  â€¢ banking_server.public.accounts
  â€¢ banking_server.public.transactions
```

**Key concept:** `snapshot.mode: initial` means Debezium first captures ALL existing data in the tables, then continues capturing new changes.

**How to run:**
```bash
python kafka/generate_and_post.py
# Output: âœ… Connector created successfully!
```

---

### Phase 6: Consuming Data (Kafka â†’ S3)

#### Step 6 â†’ `AWS_S3/kafka_to_s3.py`
ğŸ“ **File:** `Banking_project/AWS_S3/kafka_to_s3.py`

**What to learn:** This is the final piece â€” reading events from Kafka and storing them in S3.

**What it does step by step:**
1. Connects to Kafka at `localhost:29092` (external port for Windows)
2. Subscribes to all 3 topics
3. For each event:
   - Extracts the `payload.after` field (the actual row data)
   - Buffers records in memory
4. When buffer reaches 50 records:
   - Converts to Pandas DataFrame
   - Saves as `.parquet` file
   - Uploads to S3: `s3://bucket/table_name/date=YYYY-MM-DD/file.parquet`
   - Deletes the local parquet file

**S3 folder structure:**
```
s3://banking-data-lake-samba/
â”œâ”€â”€ customers/
â”‚   â””â”€â”€ date=2026-02-10/
â”‚       â””â”€â”€ customers_143022123456.parquet
â”œâ”€â”€ accounts/
â”‚   â””â”€â”€ date=2026-02-10/
â”‚       â””â”€â”€ accounts_143025654321.parquet
â””â”€â”€ transactions/
    â””â”€â”€ date=2026-02-10/
        â””â”€â”€ transactions_143028987654.parquet
```

**Key concept:** Parquet is a columnar file format that is much faster and smaller than CSV/JSON. Data lakes use Parquet because tools like Snowflake, Spark, and Athena can read it very efficiently.

**How to run:**
```bash
python AWS_S3/kafka_to_s3.py
# Output: âœ… Connected to Kafka. Listening for messages...
# Output: [banking_server.public.customers] -> {id: 1, first_name: "John", ...}
```

---

### Phase 7: Testing (Verify Each Piece Works)

#### Step 7a â†’ `test_kafka.py`
ğŸ“ **File:** `Banking_project/test_kafka.py`

**Purpose:** Check if Kafka is working â€” lists topics, counts messages, reads sample events.

```bash
python test_kafka.py
# Output: âœ… Topic 'banking_server.public.customers' exists!
# Output: Total messages: 150
```

#### Step 7b â†’ `test_s3.py`
ğŸ“ **File:** `Banking_project/test_s3.py`

**Purpose:** Check if AWS S3 connection works â€” lists buckets, uploads a test file.

```bash
python test_s3.py
# Output: Connected to AWS S3
# Output: âœ… Test file uploaded to s3://your-bucket/test/hello.txt
```

---

## ğŸš€ Complete Startup Order (How to Run the Whole Pipeline)

```
 STEP    WHAT TO DO                           WHERE                    COMMAND
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1     Start all Docker containers          Terminal 1               docker compose up -d
  2     Wait ~30 seconds for containers to be ready
  3     Create database tables               pgAdmin/psql             Run schema.sql
  4     Register Debezium connector          Terminal 2               python kafka/generate_and_post.py
  5     (Optional) Test Kafka                Terminal 2               python test_kafka.py
  6     (Optional) Test S3                   Terminal 2               python test_s3.py
  7     Start fake data generator            Terminal 3               python data_generator/fake_generator.py
  8     Start Kafka â†’ S3 consumer            Terminal 4               python AWS_S3/kafka_to_s3.py
  9     Open Airflow UI                      Browser                  http://localhost:8080
                                                                      (admin / admin)
```

After step 8, your pipeline is fully running:
- `fake_generator.py` inserts data into Postgres every 3 seconds
- Debezium captures every INSERT and sends it to Kafka
- `kafka_to_s3.py` reads from Kafka and uploads Parquet files to S3
- Airflow orchestrates/schedules dbt transformations on Snowflake

---

## ğŸ“Š Technologies Used â€” Summary

| Technology | Role | Analogy |
|-----------|------|---------|
| **Python + Faker** | Generate fake data | The factory that makes products |
| **PostgreSQL** | Store banking data | The warehouse |
| **Debezium** | Capture every DB change | Security camera on the warehouse |
| **Apache Kafka** | Stream events | The conveyor belt |
| **Zookeeper** | Manage Kafka | The factory manager |
| **AWS S3** | Store data as files | The archive/cold storage |
| **Parquet** | File format | The box used for packaging |
| **dbt** | Transform data | The quality inspector |
| **Snowflake** | Cloud data warehouse | The final showroom |
| **Apache Airflow** | Orchestrate everything | The shift supervisor |
| **Docker Compose** | Run all services | The power switch for the factory |
