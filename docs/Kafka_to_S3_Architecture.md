# Banking CDC Pipeline - Complete Architecture Guide

> **Project Goal**: Capture real-time changes from PostgreSQL database and stream them to AWS S3 as Parquet files for analytics.

---

## Table of Contents
1. [High-Level Architecture](#high-level-architecture)
2. [Component Deep Dive](#component-deep-dive)
3. [How Components Connect](#how-components-connect)
4. [Configuration Explained](#configuration-explained)
5. [Kafka Topics Explained](#kafka-topics-explained)
6. [Data Flow Step-by-Step](#data-flow-step-by-step)
7. [File-by-File Explanation](#file-by-file-explanation)
8. [Execution Guide](#execution-guide)

---

## High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              BANKING CDC PIPELINE                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚   Python    â”‚    â”‚  PostgreSQL â”‚    â”‚  Debezium   â”‚    â”‚   Apache    â”‚          â”‚
â”‚   â”‚  Generator  â”‚â”€â”€â”€â–¶â”‚  Database   â”‚â”€â”€â”€â–¶â”‚  Connect    â”‚â”€â”€â”€â–¶â”‚   Kafka     â”‚          â”‚
â”‚   â”‚             â”‚    â”‚             â”‚    â”‚             â”‚    â”‚             â”‚          â”‚
â”‚   â”‚ Generates   â”‚    â”‚ Stores Data â”‚    â”‚ Reads WAL   â”‚    â”‚ Streams     â”‚          â”‚
â”‚   â”‚ Fake Data   â”‚    â”‚ + WAL Logs  â”‚    â”‚ Changes     â”‚    â”‚ Messages    â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                    â”‚                  â”‚
â”‚                                                                    â–¼                  â”‚
â”‚                                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚   Python    â”‚            â”‚
â”‚                       â”‚   AWS S3    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Consumer   â”‚            â”‚
â”‚                       â”‚             â”‚                     â”‚             â”‚            â”‚
â”‚                       â”‚ Stores      â”‚                     â”‚ Reads Kafka â”‚            â”‚
â”‚                       â”‚ Parquet     â”‚                     â”‚ Writes S3   â”‚            â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Deep Dive

### 1. PostgreSQL Database (Port 5432)

**What it does**: Stores all banking data in tables.

**Tables Created**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CUSTOMERS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PRIMARY KEY) â”‚ first_name â”‚ last_name â”‚ email (UNIQUE)      â”‚
â”‚ 1                â”‚ John       â”‚ Doe       â”‚ john@email.com       â”‚
â”‚ 2                â”‚ Jane       â”‚ Smith     â”‚ jane@email.com       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ACCOUNTS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id â”‚ customer_id â”‚ account_type â”‚ balance  â”‚ currency           â”‚
â”‚ 1  â”‚ 1           â”‚ SAVINGS      â”‚ 500.00   â”‚ USD                â”‚
â”‚ 2  â”‚ 1           â”‚ CHECKING     â”‚ 250.00   â”‚ USD                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TRANSACTIONS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id â”‚ account_id â”‚ txn_type   â”‚ amount â”‚ related_acc_id â”‚ status â”‚
â”‚ 1  â”‚ 1          â”‚ DEPOSIT    â”‚ 100.00 â”‚ NULL           â”‚ DONE   â”‚
â”‚ 2  â”‚ 1          â”‚ TRANSFER   â”‚ 50.00  â”‚ 2              â”‚ DONE   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Special Configuration** (Why this is important):
```yaml
command: postgres -c wal_level=logical
                  -c max_wal_senders=10
                  -c max_replication_slots=10
```

| Config | What It Means |
|--------|---------------|
| `wal_level=logical` | Enables PostgreSQL to record WHAT data changed (not just that something changed) |
| `max_wal_senders=10` | Allows up to 10 processes to read the WAL log simultaneously |
| `max_replication_slots=10` | Reserves 10 slots for tracking who is reading WAL (Debezium uses one slot) |

> **WAL (Write-Ahead Log)**: PostgreSQL writes every INSERT/UPDATE/DELETE to a log file BEFORE actually changing the data. Debezium reads this log to know what changed.

---

### 2. Zookeeper (Port 2181)

**What it does**: Manages and coordinates Kafka brokers.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ZOOKEEPER                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   Responsibilities:                                              â”‚
â”‚   âœ“ Keeps track of which Kafka brokers are alive                â”‚
â”‚   âœ“ Elects a leader broker if one fails                         â”‚
â”‚   âœ“ Stores metadata about topics and partitions                 â”‚
â”‚   âœ“ Manages configuration for Kafka cluster                     â”‚
â”‚                                                                  â”‚
â”‚   Configuration:                                                 â”‚
â”‚   â€¢ ZOOKEEPER_CLIENT_PORT: 2181  â† Kafka connects here          â”‚
â”‚   â€¢ ZOOKEEPER_TICK_TIME: 2000    â† Heartbeat interval (2 sec)   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 3. Apache Kafka (Ports 9092, 29092)

**What it does**: Acts as a message queue/streaming platform. Stores and delivers messages.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          KAFKA BROKER                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   TWO LISTENERS (Two ways to connect):                          â”‚
â”‚                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ PLAINTEXT (Port 9092)                                    â”‚   â”‚
â”‚   â”‚ â€¢ For: Docker containers to talk to Kafka               â”‚   â”‚
â”‚   â”‚ â€¢ Example: Debezium Connect â†’ kafka:9092                â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ PLAINTEXT_HOST (Port 29092)                              â”‚   â”‚
â”‚   â”‚ â€¢ For: Your Python scripts on Windows to talk to Kafka  â”‚   â”‚
â”‚   â”‚ â€¢ Example: kafka_to_s3.py â†’ localhost:29092             â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration Explained**:
```yaml
KAFKA_BROKER_ID: 1
# Unique ID for this Kafka broker (if you have multiple brokers, each gets a different ID)

KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
# Tells Kafka where to find Zookeeper (using Docker service name "zookeeper")

KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
# Kafka listens on these ports
# 0.0.0.0 means "accept connections from anywhere"

KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://host.docker.internal:29092
# These are the addresses Kafka tells clients to use:
# â€¢ Docker containers should use: kafka:9092
# â€¢ Windows host should use: host.docker.internal:29092 (maps to localhost on Windows)

KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
# Internal topic __consumer_offsets has 1 copy (since we have only 1 broker)
```

---

### 4. Debezium Connect (Port 8083)

**What it does**: Reads PostgreSQL WAL log and publishes changes to Kafka topics.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       DEBEZIUM CONNECT                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   How it works:                                                  â”‚
â”‚                                                                  â”‚
â”‚   1. Connects to PostgreSQL                                      â”‚
â”‚   2. Creates a "replication slot" named "banking_slot"          â”‚
â”‚   3. Reads WAL log entries                                       â”‚
â”‚   4. Converts each change to a JSON message                      â”‚
â”‚   5. Publishes to Kafka topic                                    â”‚
â”‚                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  PostgreSQL Change:                                      â”‚   â”‚
â”‚   â”‚  INSERT INTO customers VALUES (1, 'John', 'Doe', ...)   â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â†“                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Debezium transforms to JSON:                            â”‚   â”‚
â”‚   â”‚  {                                                       â”‚   â”‚
â”‚   â”‚    "before": null,                                       â”‚   â”‚
â”‚   â”‚    "after": {                                            â”‚   â”‚
â”‚   â”‚      "id": 1,                                            â”‚   â”‚
â”‚   â”‚      "first_name": "John",                               â”‚   â”‚
â”‚   â”‚      "last_name": "Doe"                                  â”‚   â”‚
â”‚   â”‚    },                                                    â”‚   â”‚
â”‚   â”‚    "op": "c"  â† c=create, u=update, d=delete            â”‚   â”‚
â”‚   â”‚  }                                                       â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â†“                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Publishes to Kafka topic:                               â”‚   â”‚
â”‚   â”‚  banking_server.public.customers                         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Debezium Internal Topics** (Created automatically):
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          DEBEZIUM INTERNAL TOPICS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚  connect-configs                                                                â”‚
â”‚  â”œâ”€â”€ Stores: Connector configurations                                           â”‚
â”‚  â””â”€â”€ Example: postgres-connector settings like database.hostname, tables, etc. â”‚
â”‚                                                                                 â”‚
â”‚  connect-offsets                                                                â”‚
â”‚  â”œâ”€â”€ Stores: Last read position in PostgreSQL WAL                              â”‚
â”‚  â””â”€â”€ Purpose: If Debezium restarts, it knows where to continue reading from    â”‚
â”‚                                                                                 â”‚
â”‚  connect-status                                                                 â”‚
â”‚  â”œâ”€â”€ Stores: Health status of connectors                                        â”‚
â”‚  â””â”€â”€ Purpose: Track if connectors are RUNNING, PAUSED, or FAILED               â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5. AWS S3 (Simple Storage Service)

**What it does**: Cloud object storage for Parquet files.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          AWS S3                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   What it is:                                                    â”‚
â”‚   â€¢ Cloud-based object storage by Amazon                        â”‚
â”‚   â€¢ Highly durable (99.999999999% durability)                   â”‚
â”‚   â€¢ Pay-per-use pricing                                          â”‚
â”‚                                                                  â”‚
â”‚   How we use it:                                                 â”‚
â”‚   â€¢ Store Parquet files from Kafka consumer                     â”‚
â”‚   â€¢ Partitioned by date for efficient querying                  â”‚
â”‚   â€¢ Can be queried by Athena, Snowflake, etc.                   â”‚
â”‚                                                                  â”‚
â”‚   Bucket Structure:                                              â”‚
â”‚   s3://banking-data-lake/                                        â”‚
â”‚   â”œâ”€â”€ customers/date=2026-01-26/*.parquet                       â”‚
â”‚   â”œâ”€â”€ accounts/date=2026-01-26/*.parquet                        â”‚
â”‚   â””â”€â”€ transactions/date=2026-01-26/*.parquet                    â”‚
â”‚                                                                  â”‚
â”‚   Connection (boto3):                                            â”‚
â”‚   â€¢ Uses AWS_ACCESS_KEY_ID                                       â”‚
â”‚   â€¢ Uses AWS_SECRET_ACCESS_KEY                                   â”‚
â”‚   â€¢ Uses AWS_REGION                                              â”‚
â”‚   â€¢ No endpoint_url needed (unlike MinIO)                        â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## How Components Connect

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              CONNECTION MAP                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                                   DOCKER NETWORK: banking-mds-net            â”‚   â”‚
â”‚   â”‚                                                                              â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚   â”‚
â”‚   â”‚   â”‚ Postgres â”‚â—€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Debezium â”‚â—€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Kafka   â”‚                    â”‚   â”‚
â”‚   â”‚   â”‚ :5432    â”‚   TCP   â”‚ :8083    â”‚   TCP   â”‚  :9092   â”‚                    â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                    â”‚   â”‚
â”‚   â”‚                                                   â”‚                          â”‚   â”‚
â”‚   â”‚                                                   â”‚ kafka:9092               â”‚   â”‚
â”‚   â”‚                                                   â–¼                          â”‚   â”‚
â”‚   â”‚                                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚   â”‚
â”‚   â”‚                                             â”‚Zookeeper â”‚                    â”‚   â”‚
â”‚   â”‚                                             â”‚  :2181   â”‚                    â”‚   â”‚
â”‚   â”‚                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                        â”‚                                             â”‚
â”‚                                        â”‚ Port 29092 exposed to Windows               â”‚
â”‚                                        â–¼                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                              WINDOWS HOST (Your PC)                          â”‚   â”‚
â”‚   â”‚                                                                              â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚
â”‚   â”‚   â”‚ fake_generator â”‚â”€â”€â–¶ localhost:5432 â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Postgres     â”‚           â”‚   â”‚
â”‚   â”‚   â”‚     .py        â”‚   (Python to Postgres)    â”‚   (Docker)     â”‚           â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚
â”‚   â”‚                                                                              â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚
â”‚   â”‚   â”‚ kafka_to_s3.py â”‚â”€â”€â–¶ localhost:29092 â”€â”€â”€â”€â”€â”€â–¶â”‚    Kafka       â”‚           â”‚   â”‚
â”‚   â”‚   â”‚                â”‚   (Python to Kafka)       â”‚   (Docker)     â”‚           â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚
â”‚   â”‚          â”‚                                                                   â”‚   â”‚
â”‚   â”‚          â–¼                                                                   â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                        â”‚   â”‚
â”‚   â”‚   â”‚    AWS S3      â”‚ â—€â”€â”€ Parquet files via boto3                            â”‚   â”‚
â”‚   â”‚   â”‚   (Cloud)      â”‚                                                        â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                        â”‚   â”‚
â”‚   â”‚                                                                              â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Configuration Explained

### docker-compose.yml - Service by Service

```yaml
# SERVICE 1: ZOOKEEPER
zookeeper:
  image: confluentinc/cp-zookeeper:7.4.0    # Official Confluent Zookeeper image
  environment:
    ZOOKEEPER_CLIENT_PORT: 2181              # Port where Zookeeper listens
    ZOOKEEPER_TICK_TIME: 2000                # Heartbeat interval in milliseconds
  ports:
    - "2181:2181"                            # Expose port 2181 to host
```

```yaml
# SERVICE 2: KAFKA
kafka:
  image: confluentinc/cp-kafka:7.4.1
  depends_on:
    - zookeeper                              # Start Zookeeper first!
  ports:
    - "9092:9092"                            # Internal (Docker-to-Docker)
    - "29092:29092"                          # External (Windows-to-Docker)
  environment:
    KAFKA_BROKER_ID: 1                       # Unique broker ID
    KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181  # How to find Zookeeper

    # LISTENERS: What Kafka accepts connections on
    KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
    #               â””â”€ Docker internal â”€â”˜     â””â”€ Windows host â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    # ADVERTISED: What Kafka tells clients to connect to
    KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://host.docker.internal:29092
    #                           â””â”€ Docker uses this â”€â”˜  â””â”€ Windows uses this â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
    # Maps listener names to security protocols (both use PLAINTEXT = no encryption)

    KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
    # Brokers talk to each other using PLAINTEXT listener

    # REPLICATION: How many copies of data to keep
    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1            # __consumer_offsets topic
    KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1    # Transaction log
    KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1               # Minimum in-sync replicas
    # All set to 1 because we have only 1 broker
```

```yaml
# SERVICE 3: DEBEZIUM CONNECT
connect:
  image: debezium/connect:2.2
  depends_on:
    - kafka
    - zookeeper
    - postgres
  ports:
    - "8083:8083"                            # REST API port
  environment:
    BOOTSTRAP_SERVERS: 'kafka:9092'          # Connect to Kafka (Docker internal)
    GROUP_ID: '1'                            # Connect worker group ID

    # WHERE DEBEZIUM STORES ITS STATE (as Kafka topics)
    CONFIG_STORAGE_TOPIC: 'connect-configs'  # Connector configurations
    OFFSET_STORAGE_TOPIC: 'connect-offsets'  # Read positions in PostgreSQL WAL
    STATUS_STORAGE_TOPIC: 'connect-status'   # Connector health status

    # MESSAGE FORMAT (JSON without schema)
    KEY_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
    VALUE_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
    KEY_CONVERTER_SCHEMAS_ENABLE: 'false'    # Don't include schema in key
    VALUE_CONVERTER_SCHEMAS_ENABLE: 'false'  # Don't include schema in value
```

---

### Debezium Connector Configuration (generate_and_post.py)

```python
connector_config = {
    "name": "postgres-connector",            # Name for this connector
    "config": {
        # CONNECTOR TYPE
        "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
        # Tells Debezium to use PostgreSQL connector (not MySQL, MongoDB, etc.)

        # DATABASE CONNECTION
        "database.hostname": os.getenv("POSTGRES_HOST"),   # localhost or docker service name
        "database.port": os.getenv("POSTGRES_PORT"),       # 5432
        "database.user": os.getenv("POSTGRES_USER"),       # postgres
        "database.password": os.getenv("POSTGRES_PASSWORD"), # ****
        "database.dbname": os.getenv("POSTGRES_DB"),       # banking

        # TOPIC NAMING
        "topic.prefix": "banking_server",
        # All topics will start with this prefix
        # Format: {prefix}.{schema}.{table}
        # Example: banking_server.public.customers

        # WHICH TABLES TO CAPTURE
        "table.include.list": "public.customers,public.accounts,public.transactions",
        # Only capture changes from these 3 tables

        # PostgreSQL SPECIFIC
        "plugin.name": "pgoutput",
        # WAL decoder plugin (pgoutput is built into PostgreSQL 10+)

        "slot.name": "banking_slot",
        # Replication slot name - PostgreSQL reserves a spot for Debezium to read WAL

        "publication.autocreate.mode": "filtered",
        # Automatically create a publication for the filtered tables

        # DATA HANDLING
        "tombstones.on.delete": "false",
        # Don't create extra "tombstone" messages for deletes

        "decimal.handling.mode": "double",
        # Convert DECIMAL/NUMERIC to double (easier to handle in Python)
    }
}
```

---

## Kafka Topics Explained

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              ALL KAFKA TOPICS                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                       â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚   â•‘                          DATA TOPICS (Your Data)                               â•‘  â”‚
â”‚   â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£  â”‚
â”‚   â•‘                                                                                â•‘  â”‚
â”‚   â•‘   banking_server.public.customers                                              â•‘  â”‚
â”‚   â•‘   â”œâ”€â”€ Contains: All INSERT/UPDATE/DELETE on customers table                   â•‘  â”‚
â”‚   â•‘   â””â”€â”€ Message example:                                                         â•‘  â”‚
â”‚   â•‘       {                                                                        â•‘  â”‚
â”‚   â•‘         "payload": {                                                           â•‘  â”‚
â”‚   â•‘           "before": null,                                                      â•‘  â”‚
â”‚   â•‘           "after": {"id": 1, "first_name": "John", "last_name": "Doe", ...}   â•‘  â”‚
â”‚   â•‘         }                                                                      â•‘  â”‚
â”‚   â•‘       }                                                                        â•‘  â”‚
â”‚   â•‘                                                                                â•‘  â”‚
â”‚   â•‘   banking_server.public.accounts                                               â•‘  â”‚
â”‚   â•‘   â”œâ”€â”€ Contains: All changes to accounts table                                  â•‘  â”‚
â”‚   â•‘   â””â”€â”€ Message example:                                                         â•‘  â”‚
â”‚   â•‘       {                                                                        â•‘  â”‚
â”‚   â•‘         "payload": {                                                           â•‘  â”‚
â”‚   â•‘           "after": {"id": 1, "customer_id": 1, "balance": 500.00, ...}        â•‘  â”‚
â”‚   â•‘         }                                                                      â•‘  â”‚
â”‚   â•‘       }                                                                        â•‘  â”‚
â”‚   â•‘                                                                                â•‘  â”‚
â”‚   â•‘   banking_server.public.transactions                                           â•‘  â”‚
â”‚   â•‘   â”œâ”€â”€ Contains: All changes to transactions table                              â•‘  â”‚
â”‚   â•‘   â””â”€â”€ Message example:                                                         â•‘  â”‚
â”‚   â•‘       {                                                                        â•‘  â”‚
â”‚   â•‘         "payload": {                                                           â•‘  â”‚
â”‚   â•‘           "after": {"id": 1, "account_id": 1, "txn_type": "DEPOSIT", ...}     â•‘  â”‚
â”‚   â•‘         }                                                                      â•‘  â”‚
â”‚   â•‘       }                                                                        â•‘  â”‚
â”‚   â•‘                                                                                â•‘  â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                                       â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚   â•‘                     DEBEZIUM INTERNAL TOPICS                                   â•‘  â”‚
â”‚   â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£  â”‚
â”‚   â•‘                                                                                â•‘  â”‚
â”‚   â•‘   connect-configs                                                              â•‘  â”‚
â”‚   â•‘   â””â”€â”€ Stores connector configurations (postgres-connector settings)           â•‘  â”‚
â”‚   â•‘                                                                                â•‘  â”‚
â”‚   â•‘   connect-offsets                                                              â•‘  â”‚
â”‚   â•‘   â””â”€â”€ Stores WAL read position (so Debezium can resume after restart)        â•‘  â”‚
â”‚   â•‘                                                                                â•‘  â”‚
â”‚   â•‘   connect-status                                                               â•‘  â”‚
â”‚   â•‘   â””â”€â”€ Stores connector status (RUNNING, PAUSED, FAILED)                       â•‘  â”‚
â”‚   â•‘                                                                                â•‘  â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                                       â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚   â•‘                      KAFKA INTERNAL TOPICS                                     â•‘  â”‚
â”‚   â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£  â”‚
â”‚   â•‘                                                                                â•‘  â”‚
â”‚   â•‘   __consumer_offsets                                                           â•‘  â”‚
â”‚   â•‘   â””â”€â”€ Tracks which messages each consumer group has read                       â•‘  â”‚
â”‚   â•‘       Example: Consumer "banking-s3-consumer" read up to offset 150           â•‘  â”‚
â”‚   â•‘                                                                                â•‘  â”‚
â”‚   â•‘   __transaction_state                                                          â•‘  â”‚
â”‚   â•‘   â””â”€â”€ Tracks transaction states for exactly-once processing                   â•‘  â”‚
â”‚   â•‘                                                                                â•‘  â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Flow Step-by-Step

### Step 1: Generate Fake Data

```
fake_generator.py
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   cur.execute("INSERT INTO customers (first_name, ...) ...")    â”‚
â”‚                                                                  â”‚
â”‚   What happens:                                                  â”‚
â”‚   1. Python sends INSERT to PostgreSQL                          â”‚
â”‚   2. PostgreSQL writes to WAL: "INSERT id=1, name=John..."      â”‚
â”‚   3. PostgreSQL writes to actual table                          â”‚
â”‚   4. Returns success to Python                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 2: Debezium Captures Change

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Debezium (running continuously):                               â”‚
â”‚                                                                  â”‚
â”‚   1. Reads WAL log via replication slot "banking_slot"          â”‚
â”‚   2. Sees: INSERT into customers, id=1, name=John               â”‚
â”‚   3. Creates JSON message:                                       â”‚
â”‚      {                                                           â”‚
â”‚        "schema": {...},                                          â”‚
â”‚        "payload": {                                              â”‚
â”‚          "before": null,        â† Old row (null for INSERT)     â”‚
â”‚          "after": {             â† New row data                   â”‚
â”‚            "id": 1,                                              â”‚
â”‚            "first_name": "John",                                 â”‚
â”‚            "last_name": "Doe",                                   â”‚
â”‚            "email": "john@email.com"                             â”‚
â”‚          },                                                      â”‚
â”‚          "source": {...},       â† Metadata about the change     â”‚
â”‚          "op": "c",             â† Operation: c=create            â”‚
â”‚          "ts_ms": 1706234567890 â† Timestamp                      â”‚
â”‚        }                                                         â”‚
â”‚      }                                                           â”‚
â”‚   4. Publishes to topic: banking_server.public.customers        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 3: Consumer Reads from Kafka

```
kafka_to_s3.py
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   consumer = KafkaConsumer(                                      â”‚
â”‚       'banking_server.public.customers',                         â”‚
â”‚       'banking_server.public.accounts',                          â”‚
â”‚       'banking_server.public.transactions',                      â”‚
â”‚       bootstrap_servers='localhost:29092',                       â”‚
â”‚       group_id='banking-s3-consumer'                            â”‚
â”‚   )                                                              â”‚
â”‚                                                                  â”‚
â”‚   for message in consumer:                                       â”‚
â”‚       # message.value contains the JSON from Debezium            â”‚
â”‚       # Extract: payload â†’ after (the actual row data)          â”‚
â”‚       record = message.value["payload"]["after"]                 â”‚
â”‚       buffer[topic].append(record)                               â”‚
â”‚                                                                  â”‚
â”‚       # When buffer reaches 50 records, write to S3              â”‚
â”‚       if len(buffer[topic]) >= 50:                               â”‚
â”‚           write_to_s3(...)                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 4: Write to AWS S3

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   write_to_s3(table_name, records):                              â”‚
â”‚                                                                  â”‚
â”‚   1. Convert records to Pandas DataFrame                         â”‚
â”‚   2. Save as Parquet file locally (temporary)                    â”‚
â”‚   3. Upload to AWS S3:                                           â”‚
â”‚      s3://banking-data-lake/                                     â”‚
â”‚          â””â”€â”€ customers/                                          â”‚
â”‚              â””â”€â”€ date=2026-01-26/                                â”‚
â”‚                  â””â”€â”€ customers_143025123456.parquet              â”‚
â”‚   4. Delete local temporary file                                 â”‚
â”‚                                                                  â”‚
â”‚   Uses boto3 client with:                                        â”‚
â”‚   â€¢ AWS_ACCESS_KEY_ID                                            â”‚
â”‚   â€¢ AWS_SECRET_ACCESS_KEY                                        â”‚
â”‚   â€¢ AWS_REGION                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## File-by-File Explanation

### ðŸ“ Project Structure

```
banking_project/
â”œâ”€â”€ .env                          # All environment variables
â”œâ”€â”€ docker-compose.yml            # Docker services configuration
â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚
â”œâ”€â”€ data_generator/
â”‚   â””â”€â”€ fake_generator.py         # Generates fake banking data
â”‚
â”œâ”€â”€ kafka/
â”‚   â””â”€â”€ generate_and_post.py      # Registers Debezium connector
â”‚
â”œâ”€â”€ AWS_S3/
â”‚   â””â”€â”€ kafka_to_s3.py            # Consumes Kafka, writes to S3
â”‚
â””â”€â”€ postgres/
    â””â”€â”€ init.sql                  # Database schema (tables)
```

### ðŸ“„ .env (Environment Variables)

```env
# PostgreSQL - Where fake_generator.py connects
POSTGRES_USER=postgres
POSTGRES_PASSWORD=samba1004
POSTGRES_DB=banking
POSTGRES_PORT=5432
POSTGRES_HOST=localhost

# Kafka - Where kafka_to_s3.py connects
KAFKA_BOOTSTRAP=localhost:29092
KAFKA_GROUP=banking-s3-consumer

# AWS S3 - Where parquet files go (REQUIRED)
AWS_ACCESS_KEY_ID=AKIAXXXXXXXXXXXXXXXX
AWS_SECRET_ACCESS_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
AWS_REGION=us-east-1
S3_BUCKET_NAME=banking-data-lake
```

> **How to get AWS credentials:**
> 1. Go to AWS Console â†’ IAM â†’ Users â†’ Your User â†’ Security Credentials
> 2. Create Access Key â†’ Download CSV
> 3. Copy values to .env file

---

## Execution Guide

### Complete Startup Sequence

```
STEP 1: Start Docker Services
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
> docker-compose up -d

What starts:
âœ“ Zookeeper     (port 2181)
âœ“ Kafka         (ports 9092, 29092)
âœ“ PostgreSQL    (port 5432)
âœ“ Debezium      (port 8083)


STEP 2: Wait for Services to be Ready (~30 seconds)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
> curl http://localhost:8083/connectors
Should return: []


STEP 3: Create Database Tables
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
> psql -h localhost -U postgres -d banking -f postgres/init.sql


STEP 4: Register Debezium Connector
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
> python kafka/generate_and_post.py
Should print: âœ… Connector created successfully!


STEP 5: Verify Connector is Running
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
> curl http://localhost:8083/connectors/postgres-connector/status
Should show: "state": "RUNNING"


STEP 6: Start Data Generator
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
> python data_generator/fake_generator.py
Generates data every 3 seconds


STEP 7: Start Kafka-to-S3 Consumer
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
> python AWS_S3/kafka_to_s3.py
Reads from Kafka, writes to S3
```

### Useful Debug Commands

```bash
# List all Kafka topics
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092

# Read messages from a topic
docker exec -it kafka kafka-console-consumer \
    --bootstrap-server localhost:9092 \
    --topic banking_server.public.customers \
    --from-beginning

# Check Debezium connector status
curl http://localhost:8083/connectors/postgres-connector/status | jq

# List S3 bucket contents (AWS CLI)
aws s3 ls s3://banking-data-lake/ --recursive
```

---

## Summary Table

| Component | Port | Connection String | Purpose |
|-----------|------|-------------------|---------|
| PostgreSQL | 5432 | `localhost:5432` | Stores banking data |
| Zookeeper | 2181 | `zookeeper:2181` | Kafka coordination |
| Kafka | 29092 | `localhost:29092` | Message streaming |
| Debezium | 8083 | `http://localhost:8083` | CDC from PostgreSQL |
| AWS S3 | 443 | `s3.amazonaws.com` | Cloud storage for Parquet files |

---

## Quick Reference

```
PostgreSQL (Banking Data)
        â”‚
        â”‚ WAL (Write-Ahead Log)
        â–¼
Debezium Connect â”€â”€â”€â”€â”€â”€â–¶ Reads WAL changes
        â”‚
        â”‚ Publishes JSON messages
        â–¼
Kafka Topics:
â”œâ”€â”€ banking_server.public.customers
â”œâ”€â”€ banking_server.public.accounts
â””â”€â”€ banking_server.public.transactions
        â”‚
        â”‚ Consumes messages
        â–¼
Python Consumer (kafka_to_s3.py)
        â”‚
        â”‚ Converts to Parquet
        â–¼
AWS S3 Bucket
â””â”€â”€ table_name/date=YYYY-MM-DD/*.parquet
```
